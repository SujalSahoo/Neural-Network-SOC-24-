{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKFvjrvcqiUNFDLdOJpA0k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SujalSahoo/Neural-Network-SOC-24-/blob/main/SOC_week_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def model(w, x, b):\n",
        "    f = np.dot(w, x) + b\n",
        "    return f\n",
        "\n",
        "def sigmoid(x):\n",
        "    g = 1 / (1 + np.exp(-x))\n",
        "    return g\n",
        "\n",
        "def loss_function(x, y, shape, seed=None):\n",
        "    np.random.seed(seed)\n",
        "    w = np.random.rand(*shape)\n",
        "    b = np.random.randint(0, 50, (shape[0], 1))\n",
        "    m = len(x)\n",
        "    J = 0\n",
        "    a = np.zeros((m, 1))\n",
        "\n",
        "    for i in range(m):\n",
        "        a[i] = model(w, x[i], b)\n",
        "        a[i] = sigmoid(a[i])\n",
        "        J += (y[i] - a[i])**2\n",
        "\n",
        "    J = J / (2 * m)\n",
        "    return J\n",
        "\n",
        "# Example usage\n",
        "seed = 42\n",
        "shape = (1, 1)\n",
        "random_inputs = np.random.randn(4, 1)\n",
        "random_labels = np.random.randint(0, 2, (4, 1))\n",
        "loss = loss_function(random_inputs, random_labels, shape, seed)\n",
        "print(\"Loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LHgWVUTl3zk",
        "outputId": "ecc7fa52-3ca5-4039-8354-e27bc6729353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: [0.375]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bold text** Back Propagation"
      ],
      "metadata": {
        "id": "XCkJUYF1wA7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def loss_function(y_pred, y_actual):\n",
        "    return np.mean((y_actual - y_pred)**2)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.W1 = np.random.rand(self.input_size, self.hidden_size)\n",
        "        self.W2 = np.random.rand(self.hidden_size, self.output_size)\n",
        "\n",
        "        self.b1 = np.zeros((1, self.hidden_size))\n",
        "        self.b2 = np.zeros((1, self.output_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "      self.z1 = np.dot(x, self.W1) + self.b1\n",
        "      self.a1 = sigmoid(self.z1)\n",
        "      self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "      self.a2 = sigmoid(self.z2)\n",
        "      return self.a2\n",
        "\n",
        "    def backward(self, x, y, learning_rate):\n",
        "      m = x.shape[0]\n",
        "      y_pred = self.forward(x)\n",
        "      d_loss = 2 * (y_pred - y) / m\n",
        "      d_z2 = d_loss * sigmoid_derivative(self.a2)\n",
        "      d_W2 = np.dot(self.a1.T, d_z2)\n",
        "      d_b2 = np.sum(d_z2, axis=0, keepdims=True)\n",
        "      d_z1 = np.dot(d_z2, self.W2.T) * sigmoid_derivative(self.a1)\n",
        "      d_W1 = np.dot(x.T, d_z1)\n",
        "      d_b1 = np.sum(d_z1, axis=0, keepdims=True)\n",
        "      self.W1 -= learning_rate * d_W1\n",
        "      self.W2 -= learning_rate * d_W2\n",
        "      self.b1 -= learning_rate * d_b1\n",
        "      self.b2 -= learning_rate * d_b2\n",
        "      return d_loss\n",
        "\n",
        "    def train_data(self, x, y, learning_rate, epochs):\n",
        "      for i in range(epochs):\n",
        "        d_loss = self.backward(x, y, learning_rate)\n",
        "        if i % 1000 == 0:\n",
        "          y_pred = self.forward(x)\n",
        "          d_loss = loss_function(y_pred, y)\n",
        "          print(\"Epoch:\", i, \"Loss:\", d_loss)\n",
        "\n",
        "#example usage\n",
        "\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "input_size = X.shape[1]\n",
        "hidden_size = 4\n",
        "output_size = 1\n",
        "\n",
        "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
        "nn.train_data(X, y, learning_rate, epochs)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWqjFcIewPYs",
        "outputId": "df48297f-faa2-4ea2-b69c-c0f7ee8a602c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 0.2985103118798832\n",
            "Epoch: 1000 Loss: 0.24631917093562494\n",
            "Epoch: 2000 Loss: 0.23801825054981976\n",
            "Epoch: 3000 Loss: 0.21699321790342957\n",
            "Epoch: 4000 Loss: 0.18468901953837624\n",
            "Epoch: 5000 Loss: 0.15112400175776963\n",
            "Epoch: 6000 Loss: 0.10478857913544457\n",
            "Epoch: 7000 Loss: 0.05891984121866819\n",
            "Epoch: 8000 Loss: 0.03302618562621308\n",
            "Epoch: 9000 Loss: 0.020526833718431652\n"
          ]
        }
      ]
    }
  ]
}